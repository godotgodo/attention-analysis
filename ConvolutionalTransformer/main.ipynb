{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('attention-analysis/preprocessed_data_extra.csv')\n",
    "\n",
    "features = [\n",
    "    'face_movement', 'body_movement', 'eye_openness_rate',\n",
    "    'eye_direction_x', 'eye_direction_y', 'mouth_openness_rate',\n",
    "    'yaw_angle', 'pitch_angle', 'roll_angle'\n",
    "]\n",
    "INPUT_SIZE = len(features)\n",
    "SEQ_LEN = 30\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for user_id in df['id'].unique():\n",
    "    user_df = df[df['id'] == user_id].copy()\n",
    "    for i in range(len(user_df) - SEQ_LEN):\n",
    "        seq = user_df[features].iloc[i:i + SEQ_LEN].values\n",
    "        # Etiket, sekansın son elemanına karşılık gelen değer olarak ayarlandı\n",
    "        label = user_df['isAttentive'].iloc[i + SEQ_LEN - 1] \n",
    "        X_seq.append(seq)\n",
    "        y_seq.append(int(label))\n",
    "\n",
    "X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
    "X_flat = X_seq.reshape((X_seq.shape[0], -1))\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_flat, y_seq)\n",
    "X_seq_balanced = X_resampled.reshape((-1, SEQ_LEN, INPUT_SIZE))\n",
    "y_seq_balanced = y_resampled\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_seq_balanced, y_seq_balanced, test_size=0.2, stratify=y_seq_balanced, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42\n",
    ")\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class ConvTransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, d_model, n_head, n_layers, dropout, kernel_size, conv_out_channels, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=input_size, \n",
    "            out_channels=conv_out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding='same' # Sekans boyutu değişmesin\n",
    "        )\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "        self.projection = nn.Linear(conv_out_channels, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_head, dropout=dropout, batch_first=True,\n",
    "            dim_feedforward=d_model*4 # Feedforward boyutunu d_model'in 4 katı olarak ayarlamak yaygın bir pratiktir\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.projection(x)\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': 0.0003281432475028239,\n",
    "    'd_model': 28,\n",
    "    'n_heads': 2,\n",
    "    'n_layers': 2,\n",
    "    'dropout': 0.1014615298421584,\n",
    "    'batch_size': 32,\n",
    "    # Not: Bu parametreler orijinal config'de yoktu, makul varsayılanlar seçildi.\n",
    "    'kernel_size': 5,\n",
    "    'conv_out_channels': 24\n",
    "}\n",
    "\n",
    "if best_params['d_model'] % best_params['n_heads'] != 0:\n",
    "    raise ValueError(\"d_model, n_heads'e tam bölünmelidir!\")\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_final = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_final = np.concatenate((y_train, y_val), axis=0)\n",
    "final_train_dataset = SequenceDataset(X_train_final, y_train_final)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "final_train_loader = DataLoader(final_train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "\n",
    "final_model = ConvTransformerClassifier(\n",
    "    input_size=INPUT_SIZE,\n",
    "    d_model=best_params['d_model'],\n",
    "    n_head=best_params['n_heads'],\n",
    "    n_layers=best_params['n_layers'],\n",
    "    dropout=best_params['dropout'],\n",
    "    kernel_size=best_params['kernel_size'],\n",
    "    conv_out_channels=best_params['conv_out_channels']\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=10, verbose=True)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "FINAL_EPOCHS = 120\n",
    "\n",
    "for epoch in range(FINAL_EPOCHS):\n",
    "    # Eğitim Aşaması\n",
    "    final_model.train()\n",
    "    total_train_loss = 0\n",
    "    for xb, yb in final_train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = final_model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(final_train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    final_model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = final_model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}/{FINAL_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Eğitim Kaybı (Training Loss)')\n",
    "plt.plot(val_losses, label='Doğrulama Kaybı (Validation Loss)')\n",
    "plt.title('Eğitim ve Doğrulama Kaybı Grafiği (Convolutional Transformer)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "final_model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        outputs = final_model(xb)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        y_true.extend(yb.numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['Dikkatsiz (0)', 'Dikkatli (1)']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# --- 7. Model ve Scaler'ı Kaydetme ---\n",
    "print(\"\\n--- Model ve Scaler kaydediliyor... ---\")\n",
    "torch.save(final_model.state_dict(), \"conv_transformer_model.pt\")\n",
    "joblib.dump(scaler, \"minmax_scaler_conv.pkl\")\n",
    "print(\"Model 'conv_transformer_model.pt' olarak kaydedildi.\")\n",
    "print(\"Scaler 'minmax_scaler_conv.pkl' olarak kaydedildi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "               precision    recall  f1-score   support\n",
    "\n",
    "Dikkatsiz (0)       0.86      0.83      0.85       144\n",
    " Dikkatli (1)       0.84      0.86      0.85       143\n",
    "\n",
    "     accuracy                           0.85       287\n",
    "    macro avg       0.85      0.85      0.85       287\n",
    " weighted avg       0.85      0.85      0.85       287\n",
    "\n",
    "\n",
    "Confusion Matrix:\n",
    "[[120  24]\n",
    " [ 20 123]]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
