{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('attention-analysis/preprocessed_data_extra.csv')\n",
    "\n",
    "features = [\n",
    "    'face_movement', 'body_movement', 'eye_openness_rate',\n",
    "    'eye_direction_x', 'eye_direction_y', 'mouth_openness_rate',\n",
    "    'yaw_angle', 'pitch_angle', 'roll_angle'\n",
    "]\n",
    "INPUT_SIZE = len(features)\n",
    "SEQ_LEN = 30\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for user_id in df['id'].unique():\n",
    "    user_df = df[df['id'] == user_id].copy()\n",
    "    for i in range(len(user_df) - SEQ_LEN):\n",
    "        seq = user_df[features].iloc[i:i + SEQ_LEN].values\n",
    "        label = user_df['isAttentive'].iloc[i + SEQ_LEN - 1]\n",
    "        X_seq.append(seq)\n",
    "        y_seq.append(int(label))\n",
    "\n",
    "X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
    "X_flat = X_seq.reshape((X_seq.shape[0], -1))\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_flat, y_seq)\n",
    "X_seq_balanced = X_resampled.reshape((-1, SEQ_LEN, INPUT_SIZE))\n",
    "y_seq_balanced = y_resampled\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_seq_balanced, y_seq_balanced, test_size=0.2, stratify=y_seq_balanced, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42\n",
    ")\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class CausalTransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, d_model, n_head, n_layers, dim_feedforward, dropout, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, SEQ_LEN, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_head, dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
    "        \"\"\"Gelecek adımların görülmesini engelleyen bir maske oluşturur.\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        src = self.input_projection(src) * math.sqrt(self.d_model)\n",
    "        src = src + self.pos_encoder\n",
    "        \n",
    "        mask = self._generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        output = self.transformer_encoder(src, mask=mask)\n",
    "        \n",
    "        output = output[:, -1, :]\n",
    "        return self.classifier(output)\n",
    "\n",
    "# Optuna'dan elde edilen en iyi parametreleri buraya doğrudan yazıyoruz.\n",
    "best_params = {\n",
    "    'learning_rate': 0.000165359106482521,\n",
    "    'd_model': 48,\n",
    "    'n_heads': 2,\n",
    "    'n_layers': 3,\n",
    "    'dropout': 0.1642622418497776,\n",
    "    'batch_size': 64\n",
    "}\n",
    "print(\"\\n--- En İyi Parametrelerle Final Model Eğitiliyor ---\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_final = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_final = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "final_train_dataset = SequenceDataset(X_train_final, y_train_final)\n",
    "val_dataset = SequenceDataset(X_val, y_val) # Validation seti kayıp takibi için kullanılacak\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "final_train_loader = DataLoader(final_train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "\n",
    "final_model = CausalTransformerClassifier(\n",
    "    input_size=INPUT_SIZE,\n",
    "    d_model=best_params['d_model'],\n",
    "    n_head=best_params['n_heads'],\n",
    "    n_layers=best_params['n_layers'],\n",
    "    dim_feedforward=best_params['d_model'] * 4,\n",
    "    dropout=best_params['dropout'],\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=10, verbose=True)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "FINAL_EPOCHS = 120\n",
    "\n",
    "for epoch in range(FINAL_EPOCHS):\n",
    "    # Eğitim Aşaması\n",
    "    final_model.train()\n",
    "    total_train_loss = 0\n",
    "    for xb, yb in final_train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = final_model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(final_train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    final_model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = final_model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    scheduler.step(avg_val_loss) # Scheduler'ı validation kaybına göre güncelle\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}/{FINAL_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Eğitim Kaybı (Training Loss)')\n",
    "plt.plot(val_losses, label='Doğrulama Kaybı (Validation Loss)')\n",
    "plt.title('Eğitim ve Doğrulama Kaybı Grafiği (Causal Transformer)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Final Test Raporu ---\")\n",
    "final_model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        outputs = final_model(xb)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        y_true.extend(yb.numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['Dikkatsiz (0)', 'Dikkatli (1)']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "torch.save(final_model.state_dict(), \"causal_transformer_model.pt\")\n",
    "joblib.dump(scaler, \"minmax_scaler_causal.pkl\")\n",
    "print(\"Model 'causal_transformer_model.pt' olarak kaydedildi.\")\n",
    "print(\"Scaler 'minmax_scaler_causal.pkl' olarak kaydedildi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Final Test Raporu ---\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "Dikkatsiz (0)       0.81      0.80      0.80       144\n",
    " Dikkatli (1)       0.80      0.81      0.81       143\n",
    "\n",
    "     accuracy                           0.80       287\n",
    "    macro avg       0.80      0.80      0.80       287\n",
    " weighted avg       0.80      0.80      0.80       287\n",
    "\n",
    "\n",
    "Confusion Matrix:\n",
    "[[115  29]\n",
    " [ 27 116]]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
